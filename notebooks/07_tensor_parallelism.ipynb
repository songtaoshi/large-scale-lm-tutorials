{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor Parallelism\n",
    "이번 세션에서는 Tensor parallelism에 대해서 알아보겠습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Intra-layer model parallelism\n",
    "Tensor Parallelism은 Intra-layer 모델 병렬화 방식으로 **레이어 내부에서 텐서 단위로 모델을 쪼갭니다.** Inter-layer 모델 병렬화는 상식적으로 이해가 가지만, Intra-layer 병렬화의 경우는 처음 보시는 분들은 어떻게 이 것이 가능한지 궁금하실거에요.\n",
    "\n",
    "![](../images/intra_layer.png)\n",
    "\n",
    "우리가 흔히 사용하는 내적 연산은 연산하고자 하는 행렬을 쪼개서 병렬적으로 수행하고 결과를 더하거나 이어붙여도 최종 출력값이 변하지 않는 성질이 있습니다. 이러한 내적 연산의 성질을 이용하여 모델을 병렬화 하는것을 Tensor 병렬화라고 합니다. 용어가 다소 헷갈릴 수 있는데 Intra-layer는 레이어 단위에서 일어나지 않는 모든 병렬화를 의미하기 때문에 더 큰 범주이고, Tensor 병렬화는 Intra-layer 병렬화의 구현하는 방법 중 한가지 입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Megatron-LM\n",
    "Megatron-LM은 NVIDA에서 공개한 Intra-layer 모델 병렬화 구현체로, 현재 Large-scale 모델 개발에 있어서 가장 중요한 프로젝트 중 하나입니다.\n",
    "\n",
    "<img src=\"../images/megatron_lm.jpeg\" width=540>\n",
    "\n",
    "### Column & Row parallelism\n",
    "다음은 Megatron-LM에서 사용되는 column parallelism과 row parallelism을 그림으로 나타낸 것입니다.\n",
    "\n",
    "- Column parallelism은 **모델의 파라미터(A)를 수직방향으로 분할(A1, A2)하는 방법**입니다.\n",
    "- Row parallelism은 **모델의 파라미터(A)를 수평방향으로 분할(A1, A2)하는 방법**입니다.\n",
    "\n",
    "![](../images/intra_layer_2.png)\n",
    "\n",
    "직접 코딩해서 결과를 확인해봅시다. 가장 먼저 텐서 X와 텐서 A의 행렬곱 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 74,  98],\n",
      "        [258, 346]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/non_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3],\n",
    "        [4, 5, 6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A = torch.tensor(\n",
    "    [\n",
    "        [10, 14],\n",
    "        [11, 15],\n",
    "        [12, 16],\n",
    "        [13, 17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y = X @ A\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "column parallelism은 모델의 파라미터(A)를 수직방향으로 자른 뒤 연산후 연산 결과를 concat하는 방식입니다. 그림에서와 같이 X는 복제하고 텐서 A를 수직방향으로 분할한 뒤 연산 후 concat 해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 74],\n",
      "        [258]])\n",
      "tensor([[ 98],\n",
      "        [346]])\n",
      "tensor([[ 74,  98],\n",
      "        [258, 346]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/column_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X = torch.tensor(\n",
    "    [\n",
    "        [0, 1, 2, 3],\n",
    "        [4, 5, 6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A1 = torch.tensor(\n",
    "    [\n",
    "        [10],\n",
    "        [11],\n",
    "        [12],\n",
    "        [13],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "A2 = torch.tensor(\n",
    "    [\n",
    "        [14],\n",
    "        [15],\n",
    "        [16],\n",
    "        [17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y1 = X @ A1\n",
    "Y2 = X @ A2\n",
    "\n",
    "print(Y1)\n",
    "print(Y2)\n",
    "\n",
    "Y = torch.cat([Y1, Y2], dim=1)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "병렬화 전 후의 연산 결과가 동일한 것을 확인 할 수 있습니다. \n",
    "\n",
    "그 다음으로 row parallelism를 알아봅시다. row parallelism은 모델의 파라미터(A)를 수평방향으로 분할 한 뒤 연산 결과를 더하는 방식입니다. 그림과 같이 X와 Y 모두를 분할한 뒤 연산 후 결과 값을 더해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 11,  15],\n",
      "        [ 95, 131]])\n",
      "tensor([[ 63,  83],\n",
      "        [163, 215]])\n",
      "tensor([[ 74,  98],\n",
      "        [258, 346]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/row_parallelism.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "\n",
    "X1 = torch.tensor(\n",
    "    [\n",
    "        [0, 1],\n",
    "        [4, 5],\n",
    "    ]\n",
    ")\n",
    "\n",
    "X2 = torch.tensor(\n",
    "    [\n",
    "        [2, 3],\n",
    "        [6, 7],\n",
    "    ]\n",
    ")\n",
    "\n",
    "A1 = torch.tensor(\n",
    "    [\n",
    "        [10, 14],\n",
    "        [11, 15],      \n",
    "    ]\n",
    ")\n",
    "\n",
    "A2 = torch.tensor(\n",
    "    [\n",
    "        [12, 16],\n",
    "        [13, 17],        \n",
    "    ]\n",
    ")\n",
    "\n",
    "Y1 = X1 @ A1\n",
    "Y2 = X2 @ A2\n",
    "\n",
    "print(Y1)\n",
    "print(Y2)\n",
    "\n",
    "Y = Y1 + Y2\n",
    "\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "연산 결과가 동일한 것을 확인할 수 있습니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### Column parallelism: $(D, D) → (D, \\frac{D}{n}) \\times n$\n",
    "\n",
    "앞선 예시에서 본 것 처럼, Column Parallelism은 **입력텐서(X)를 복사**하고, 모델의 파라미터(A)를 **수직방향으로 분할(A1, A2)하여 내적** 후 concat하는 연산입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/column_parallel.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "Megatron-LM에서는 **분할된 파라미터 (A1, A2)를 서로 다른 디바이스에 올려서 모델을 병렬화** 합니다. 이에 따라 행렬 곱 연산도 여러개의 GPU에서 동시에 일어나게 되고, 이를 처리하기 위해 분산 프로그래밍이 필요합니다. Column Parallelism을 위해서는 Broadcast와 All-gather 연산을 사용합니다.\n",
    "\n",
    "- 서로 다른 GPU에 동일한 입력을 전송하기 위해 **Broadcast** 연산를 사용합니다.\n",
    "- 행렬 곱 연산 결과를 모으기 위해 **All-gather** 연산을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고: ColumnParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    bias = self.bias if not self.skip_bias_add else None\n",
    "\n",
    "    # Set up backprop all-reduce.\n",
    "    input_parallel = copy_to_tensor_model_parallel_region(input_)\n",
    "\n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight, bias)\n",
    "\n",
    "    if self.gather_output:\n",
    "        output = gather_from_tensor_model_parallel_region(output_parallel)\n",
    "    else:\n",
    "        output = output_parallel\n",
    "    \n",
    "    output_bias = self.bias if self.skip_bias_add else None\n",
    "    return output, output_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row parallelism: $(D, D) → (\\frac{D}{n}, D) \\times n$\n",
    "\n",
    "Row Parallelism은 **입력텐서(X)를 분할**하고, 모델의 파라미터(A)를 **수평방향으로 분할(A1, A2)하여 내적** 후 더하는 연산입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/row_parallelism.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "마찬가지로 Row Parallelism을 여러 GPU에서 실행하기 위해서는 분산 프로그래밍이 필요합니다. Row Parallelism을 위해서는 Scatter와 All-reduce을 사용합니다.\n",
    "\n",
    "- 서로 다른 GPU에 입력을 분할하여 전송하기 위해 **Scatter** 연산를 사용합니다.\n",
    "- 행렬 곱 연산 결과를 더하기 위해서 **All-reduce** 연산을 사용합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고: RowParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    # Set up backprop all-reduce.\n",
    "    if self.input_is_parallel:\n",
    "        input_parallel = input_\n",
    "    else:\n",
    "        input_parallel = scatter_to_tensor_model_parallel_region(input_)\n",
    "    \n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight)\n",
    "    \n",
    "    # All-reduce across all the partitions.\n",
    "    output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n",
    "    \n",
    "    if not self.skip_bias_add:\n",
    "        output = output_ + self.bias if self.bias is not None else output_\n",
    "        output_bias = None\n",
    "    else:\n",
    "        output = output_\n",
    "        output_bias = self.bias\n",
    "    return output, output_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Transformer Block\n",
    "\n",
    "이제 Column, Row parallelism에 대해 이해했으니 본격적으로 어떻게 Transformer를 병렬화 할지 살펴봅시다. 우리가 흔히 아는 Transformer Block은 다음과 같이 구성되어 있습니다. Megatron-LM은 여기에서 파라미터의 크기가 매우 적은 Layer Norm 레이어는 파라미터를 모든 디바이스로 복제하고, Layer Norm 레이어를 제외한 다른 레이어들(Attention, MLP)은 위와 같이 Column, Row parallelism을 통해 병렬처리를 수행합니다.\n",
    "\n",
    "![](../images/megatron_block.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### MLP Layer\n",
    "\n",
    "가장 먼저 MLP 레이어에 대해 알아보겠습니다. MLP 레이어는 `Linear1` → `GeLU` → `Linear2` → `Dropout`순으로 진행됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp.png)\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고 transformers/models/gpt_neo/modeling_gpt_neo.py\n",
    "\"\"\"\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GPTNeoMLP(nn.Module):\n",
    "    def __init__(self, intermediate_size, config):  # in MLP: intermediate_size= 4 * hidden_size\n",
    "        super().__init__()\n",
    "        embed_dim = config.hidden_size\n",
    "        self.c_fc = nn.Linear(embed_dim, intermediate_size)\n",
    "        self.c_proj = nn.Linear(intermediate_size, embed_dim)\n",
    "        self.act = ACT2FN[config.activation_function]\n",
    "        self.dropout = nn.Dropout(config.resid_dropout)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.c_fc(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.c_proj(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        return hidden_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기에서 **첫번째 Linear는 Coulmn Parallelism**을, **두번째 Linear는 Row Parallelism**을 적용합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "MLP 레이어에서 Column-Row 순으로 병렬화를 적용하는 이유는 두가지가 있습니다.\n",
    "\n",
    "- 첫번째 이유는 **`All-gather` 연산과 `Scatter` 연산을 생략** 할 수 있기 때문입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "왼쪽 녹색 영역의 연산 결과는 입 력데이터 X와 각 디바이스로 병렬화된 W를 내적한 것입니다. 그리고 나서 붉은색 영역에서 이 결과값을 `All-gather`해서 이어붙인 다음에 다시 `Scatter`하여 쪼개죠. 여기에서 흥미로운 사실은 이어 붙인 텐서를 다시 쪼갰기 때문에 이는 이어붙이기 전과 동일하다는 것입니다.  따라서 오른쪽의 녹색 영역과 왼쪽의 녹색영역 값은 동일하죠. 결과적으로 붉은색 영역 (`All-gather`-`Scatter`)을 생략할 수 있고, 속도 면에서 큰 이득을 가져올 수 있습니다. \n",
    "\n",
    "이는 Column-Row 순으로 병렬화 할때만 나타나는 독특한 현상으로, 만약 Column-Column, Row-Column, Row-Row와 같이 병렬화 한다면 두 Linear 레이어 사이에서 발생하는 통신을 생략할 수 없게 됩니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_4.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "`All-gather`와 `Scatter`를 생략하는 기법은 Megatron-LM에 `input_is_parallel`와 `gather_output`라는 파라미터로 구현되어있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고: ColumnParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    bias = self.bias if not self.skip_bias_add else None\n",
    "\n",
    "    # Set up backprop all-reduce.\n",
    "    input_parallel = copy_to_tensor_model_parallel_region(input_)\n",
    "\n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight, bias)\n",
    "\n",
    "    # gather_output을 False로 설정하여 output을 병렬화된 채로 출력합니다.\n",
    "    if self.gather_output:\n",
    "        output = gather_from_tensor_model_parallel_region(output_parallel)\n",
    "    else:\n",
    "        output = output_parallel\n",
    "\n",
    "    output_bias = self.bias if self.skip_bias_add else None\n",
    "    return output, output_bias\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "참고: RowParallelLinear in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    # Set up backprop all-reduce.\n",
    "\n",
    "    # input_is_parallel True로 설정하여 input을 병렬화된 채로 입력받습니다.\n",
    "    if self.input_is_parallel:\n",
    "        input_parallel = input_\n",
    "    else:\n",
    "        input_parallel = scatter_to_tensor_model_parallel_region(input_)\n",
    "    \n",
    "    # Matrix multiply.\n",
    "    output_parallel = F.linear(input_parallel, self.weight)\n",
    "    \n",
    "    # All-reduce across all the partitions.\n",
    "    output_ = reduce_from_tensor_model_parallel_region(output_parallel)\n",
    "    \n",
    "    if not self.skip_bias_add:\n",
    "        output = output_ + self.bias if self.bias is not None else output_\n",
    "        output_bias = None\n",
    "    else:\n",
    "        output = output_\n",
    "        output_bias = self.bias\n",
    "    return output, output_bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Column-Row 방식으로 병렬화하는 2번째 이유는 `Scatter`와 `All-gather`를 생략하려면 **GeLU 연산**이 병렬화된 채로 수행되어야 하기 때문입니다.\n",
    "  \n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_5.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "위 그림은 `Scatter`와 `All-gather`를 생략하지 않는 상황에서 GeLU 연산을 두 Linear 레이어 사이에 삽입한 것입니다. 만약 여기에서 두 연산을 생략하도록 구현하면 아래와 같이 GeLU 연산은 반드시 각각의 디바이스에서 이루어져야 합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/megatron_mlp_6.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "그러나 이렇게 GeLU 연산을 서로 다른 디바이스에서 하도록 병렬화 시키려면 반드시 병렬적으로 계산된 GeLU의 출력은 병렬화 되지 않은 상태에서 계산된 GeLU의 출력과 동일해야겠죠. 즉 다음과 같은 공식이 성립해야 합니다. ($\\circledcirc$ 기호는 concatenation을 의미합니다.)\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Row Paralleism: GeLU(XW1 + XW2) = GeLU(XW1) + GeLU(XW2)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Column Paralleism: GeLU(XW1 \\circledcirc XW2) = GeLU(XW1) \\circledcirc GeLU(XW2)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "문제는 위와 같은 공식이 Column Parallelism에서만 성립하고, **Row Parallelism 에서는 성립하지 않는다는 것**입니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$Row Paralleism: GeLU(XW1 + XW2) \\neq GeLU(XW1) + GeLU(XW2)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "이를 코드로 구현해서 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GeLU in RowParallelLinear same with non-parallel = False\n",
      "Is GeLU in ColumnParallelLinear same with non-parallel = True\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "src/megatron_mlp_gelu.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from torch.nn.functional import gelu\n",
    "\n",
    "\n",
    "w = torch.randn(6, 6)\n",
    "x = torch.randn(6, 6)\n",
    "\n",
    "\n",
    "class RowParallelLinear(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RowParallelLinear, self).__init__()\n",
    "        chunked = torch.chunk(w, 2, dim=0)\n",
    "\n",
    "        # row parallelized parameters\n",
    "        self.w1 = chunked[0]  # [3, 6]\n",
    "        self.w2 = chunked[1]  # [3, 6]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # GeLU(X1A1 + X2A2) != GeLU(X1A1) + GeLU(X2A2)\n",
    "        x1, x2 = torch.chunk(x, 2, dim=1)\n",
    "\n",
    "        # parallel output\n",
    "        y1 = gelu(x1 @ self.w1) + gelu(x2 @ self.w2)\n",
    "\n",
    "        # non-parallel output\n",
    "        y2 = gelu(x1 @ self.w1 + x2 @ self.w2)\n",
    "\n",
    "        return torch.all(y1 == y2)\n",
    "\n",
    "\n",
    "class ColumnParallelLinear(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColumnParallelLinear, self).__init__()\n",
    "        chunked = torch.chunk(w, 2, dim=1)\n",
    "\n",
    "        # column parallelized parameters\n",
    "        self.w1 = chunked[0]  # [6, 3]\n",
    "        self.w2 = chunked[1]  # [6, 3]\n",
    "\n",
    "    def forward(self, x):\n",
    "        # GeLU(X1A1 cat X2A2) == GeLU(X1A1) cat GeLU(X2A2)\n",
    "\n",
    "        # parallel output\n",
    "        y1 = torch.cat([gelu(x @ self.w1), gelu(x @ self.w2)], dim=1)\n",
    "\n",
    "        # non-parallel output\n",
    "        y2 = gelu(torch.cat([(x @ self.w1), (x @ self.w2)], dim=1))\n",
    "\n",
    "        return torch.all(y1 == y2)\n",
    "\n",
    "\n",
    "# Row Parallelism\n",
    "print(\"Is GeLU in RowParallelLinear same with non-parallel = \", end=\"\")\n",
    "print(RowParallelLinear()(x).item())\n",
    "\n",
    "# Column Parallelism\n",
    "print(\"Is GeLU in ColumnParallelLinear same with non-parallel = \", end=\"\")\n",
    "print(ColumnParallelLinear()(x).item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "따라서 GeLU 연산을 병렬화 시키려면 반드시 GeLU 이전의 Linear 레이어는 Column 방향으로 병렬화 되어있어야 합니다. 따라서 Column-Row 순서로 병렬화 하는 것이 가장 효율적인 방식이죠."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Multi-head Attention Layer\n",
    "\n",
    "다음으로 Multi-head Attention 레이어에 대해 알아보겠습니다. Multi-head Attention 레이어는 `Linear1` → `Split heads` → `ScaleDotProductAttention` → `Concat(Merge) heads` → `Linear2` → `Dropout` 순으로 진행됩니다.\n",
    "\n",
    "![](../images/multi_head_attention.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고 transformers/models/gpt_neo/modeling_gpt_neo.py\n",
    "\"\"\"\n",
    "\n",
    "class GPTNeoSelfAttention(nn.Module):\n",
    "    def __init__(self, config, attention_type):\n",
    "        super().__init__()\n",
    "        self.attn_dropout = nn.Dropout(config.attention_dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.resid_dropout)\n",
    "\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.num_heads = config.num_heads\n",
    "        self.head_dim = self.embed_dim // self.num_heads\n",
    "        if self.head_dim * self.num_heads != self.embed_dim:\n",
    "            raise ValueError(\n",
    "                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
    "        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        layer_past=None,\n",
    "        head_mask=None,\n",
    "        use_cache=False,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # 1. linear projection\n",
    "        query = self.q_proj(hidden_states)\n",
    "        key = self.k_proj(hidden_states)\n",
    "        value = self.v_proj(hidden_states)\n",
    "        \n",
    "        # 2. split heads\n",
    "        query = self._split_heads(query, self.num_heads, self.head_dim)\n",
    "        key = self._split_heads(key, self.num_heads, self.head_dim)\n",
    "        value = self._split_heads(value, self.num_heads, self.head_dim)\n",
    "\n",
    "        # 3. scale dot product attention\n",
    "        attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask)\n",
    "\n",
    "        # 4. concat (merge) heads\n",
    "        attn_output = self._merge_heads(attn_output, self.num_heads, self.head_dim)\n",
    "        \n",
    "        # 5. linear projection\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        \n",
    "        # 6. dropout\n",
    "        attn_output = self.resid_dropout(attn_output)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../images/megatron_attention.jpeg)\n",
    "\n",
    "<br>\n",
    "\n",
    "Megatron-LM은 Attention 레이어의 Q, K, V Linear projection과 Output projection 부분을 병렬화 합니다. 마찬가지로 Q, K, V Linear projection 부분은 Column parallelism, Output projection 부분은 Row parallelism으로 처리하여 **Column-Row의 패턴을 만듭니다.** 이를 통해 Attention 레이어에서도 MLP 레이어와 마찬가지로 `Scatter`, `All-gather` 연산을 생략 할 수 있습니다.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab Parallel Embedding\n",
    "\n",
    "Megatron LM은 Word embedding 레이어도 역시 병렬화 합니다. 독특한 점은 Vocab size dimension을 기준으로 병렬화 한다는 점입니다. 예를 들어 Vocab size가 50000인 Word embedding matrix가 있다고 가정하면 이 matrix의 사이즈는 (50000, embedding_dim)인 됩니다. Megatron-LM은 여기에서 Vocab size dimension을 기준으로 matrix를 병렬화 합니다. 이러한 독특한 병렬화 기법을 **Vocab Parallel Embedding**이라고 합니다. \n",
    "\n",
    "![](../images/vpe_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "위 그림은 병렬화를 하지 않은 상태에서의 Word embedding을 나타냅니다. 길이가 6인 시퀀스가 입력되면 [6, embedding_dim]의 사이즈를 갖는 입력 텐서를 만듭니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpe_2.png)\n",
    "\n",
    "위 그림은 Vocab parallel embedding의 작동 방식을 나타냅니다. 기존의 임베딩 매트릭스를 절반으로 쪼개서 0번부터 24999번 토큰까지 담당하는 임베딩 매트릭스와 25000번부터 50000번 토큰까지 담당하는 임베딩 매트릭스로 분할합니다. 그리고 데이터가 들어오면 **해당 매트릭스가 커버하는 범위를 넘어서는 토큰은 마스킹**하여 처리합니다. 이후에 **마스킹 처리된 부분의 벡터는 전부 0으로 초기화** 한 뒤, 두 매트릭스를 **더하면 모든 단어의 벡터를 갖고 있는 완벽한 입력 텐서**가 됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고: VocabParallelEmbedding in megatron-lm/megatron/mpu/layers.py\n",
    "\"\"\"\n",
    "\n",
    "def forward(self, input_):\n",
    "    if self.tensor_model_parallel_size > 1:\n",
    "        # Build the mask.\n",
    "        input_mask = (input_ < self.vocab_start_index) | \\\n",
    "                     (input_ >= self.vocab_end_index)\n",
    "\n",
    "        # Mask the input.\n",
    "        masked_input = input_.clone() - self.vocab_start_index\n",
    "        masked_input[input_mask] = 0\n",
    "\n",
    "    else:\n",
    "        masked_input = input_\n",
    "        # Get the embeddings.\n",
    "    \n",
    "    output_parallel = F.embedding(masked_input, self.weight,\n",
    "                                  self.padding_idx, self.max_norm,\n",
    "                                  self.norm_type, self.scale_grad_by_freq,\n",
    "                                  self.sparse)\n",
    "\n",
    "    # Mask the output embedding.\n",
    "    if self.tensor_model_parallel_size > 1:\n",
    "        output_parallel[input_mask, :] = 0.0\n",
    "    \n",
    "    # Reduce across all the model parallel GPUs.\n",
    "    output = reduce_from_tensor_model_parallel_region(output_parallel)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "그런데 여기에서 문제가 하나 발생합니다. Tensor parallelism은 반드시 짝수개의 GPU로 병렬화 되어야 하는데 52527은 짝수가 아니기 때문에 2로 나눌 수가 없습니다. 이를 위해 Word embedding matrix에 사용하지 않는 토큰을 추가하여 vocab size를 짝수로 만듭니다. 이를 `padded vocab size`라고 하며 Megatron-LM에서는 `make-vocab-size-divisible-by`이라는 argument로 vocab size를 조절할 수 있습니다. (vocab size가 설정한 값의 배수가 되도록 만듭니다.) \n",
    "\n",
    "결론적으로 Megatron-LM은 Vocab Parallel Embedding을 적용하여 메모리 효율성을 더욱 높힐 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Vocab Parallel Cross Entropy\n",
    "\n",
    "GPT2의 Causal Language Modeling이나 BERT의 Masked Language Modeling 같은 태스크는 최종 출력으로 자연어 토큰을 생성합니다. 따라서 마지막 Transformer 레이어를 거친 이후에 모델의 출력 사이즈는 (bsz, length, vocab_size)로 확장됩니다. (classification이나 tagging 같은 태스크는 해당하지 이에 않습니다.)\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/lm_head.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "이 때, 만약 입력과 출력 임베딩을 묶는다면(weight tying) Language Modeling Head (이하 LM Head)에 사용되는 Linear 레이어의 파라미터를 새로 초기화 시키는 대신 word embedding matrix를 사용하게 됩니다. 현재 공개된 Bert, GPT2, GPTNeo 등의 대부분 모델들의 출력 임베딩(LM Head)은 입력 임베딩과 묶여있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고 transformers/models/gpt_neo/modeling_gpt_neo.py\n",
    "\"\"\"\n",
    "\n",
    "class GPTNeoForCausalLM(GPTNeoPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_missing = [\n",
    "        r\"h\\.\\d+\\.attn\\.masked_bias\",\n",
    "        r\"lm_head\\.weight\",\n",
    "        r\"h\\.\\d+\\.attn\\.attention\\.bias\",\n",
    "    ]\n",
    "    _keys_to_ignore_on_save = [r\"lm_head.weight\"]\n",
    "    # 3. 그렇기 때문에 `lm_head.weight` 파라미터는 load 및 save하지 않습니다.\n",
    "    # 굳이 동일한 텐서를 두번 저장하거나 로드 할 필요 없기 때문이죠.\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.transformer = GPTNeoModel(config)\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        # 1. 언뜻 보면 nn.Linear 레이어의 파라미터를 새로 할당해서 사용하는 것 처럼 보입니다.\n",
    "\n",
    "        self.init_weights()\n",
    "        # 2. 그러나 이 메서드를 호출하면서 입력과 출력 임베딩(lm head)을 묶게 됩니다. \n",
    "        # 이 때 word embeddig matrix의 weight를 nn.Linear 레이어의 weight로 복사하게 됩니다.\n",
    "        # 복사는 deep-copy가 아닌 shallow-copy를 수행합니다. (reference가 아닌 value만 공유)\n",
    "        # 따라서 `lm_head.weight`은 word embedding과 동일한 주소 공간에 있는 하나의 텐서입니다.\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.lm_head\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.lm_head = new_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "참고 transformers/modeling_utils.py\n",
    "\"\"\"\n",
    "\n",
    "def init_weights(self):\n",
    "    \"\"\"\n",
    "    If needed prunes and maybe initializes weights.\n",
    "    \"\"\"\n",
    "    # Prune heads if needed\n",
    "    if self.config.pruned_heads:\n",
    "        self.prune_heads(self.config.pruned_heads)\n",
    "\n",
    "    if _init_weights:\n",
    "        # Initialize weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        # weight tying을 지원하는 모델은 이 메서드가 호출됨과 동시에\n",
    "        # 입력 임베딩과 출력 임베딩(= lm head)가 묶이게 됩니다.\n",
    "        self.tie_weights()\n",
    "\n",
    "\n",
    "def tie_weights(self):\n",
    "    \"\"\"\n",
    "    Tie the weights between the input embeddings and the output embeddings.\n",
    "    If the :obj:`torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning\n",
    "    the weights instead.\n",
    "    \"\"\"\n",
    "    output_embeddings = self.get_output_embeddings()\n",
    "    if output_embeddings is not None and self.config.tie_word_embeddings:\n",
    "        self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n",
    "        # 이 메서드가 호출되면서 output 임베딩(lm head)이 input 임베딩과 묶이게 됩니다.\n",
    "\n",
    "    if self.config.is_encoder_decoder and self.config.tie_encoder_decoder:\n",
    "        if hasattr(self, self.base_model_prefix):\n",
    "            self = getattr(self, self.base_model_prefix)\n",
    "        self._tie_encoder_decoder_weights(self.encoder, self.decoder, self.base_model_prefix)\n",
    "\n",
    "    for module in self.modules():\n",
    "        if hasattr(module, \"_tie_weights\"):\n",
    "            module._tie_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러나 여기서 문제가 생깁니다. 일반적으로 LM Head로 부터 출력된 Logits과 Target 데이터 사이의 Loss를 계산할 때는 다음과 같은 과정이 일어납니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpce_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "그러나 Megatron-LM은 Vocab Parallel Embedding을 사용하기 때문에 Embedding 레이어가 여러 디바이스를 걸쳐 분할되어 있습니다. 때문에 weight tying을 하게 된다면 **출력 임베딩(LM Head) 역시 여러 디바이스로 분할**되게 됩니다. 따라서 모델에서 출력되는 Logits의 사이즈는 vocab size를 분할한 사이즈가 됩니다. \n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpce_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "위 그림처럼 vocab size가 50,000이라면 원래는 (bsz, length, 50000)의 텐서가 출력되어야 하지만 위의 예시처럼 2개의 디바이스로 분할되어 있다면 (bsz, length, 25000)의 사이즈를 갖는 2개의 logits이 나오게 되며, 각 디바이스의 logits은 서로 다른 값을 갖게 될 것입니다. **이 것을 Parallel LM Logits이라고 부릅니다.** 이렇게 되면 target sentence와의 loss를 어떻게 계산해야 할까요? Traget 데이터에는 0번 부터 49999번째 토큰까지 모두 존재하는데 비해 logits의 사이즈는 그 절반밖에 되지 않으니까요.\n",
    "\n",
    "<br>\n",
    "\n",
    "![](../images/vpce_3.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "이 경우 **기존의 cross entropy가 아닌 vocab parallel cross entropy라고 불리는 특별한 loss 함수를 사용**해야 합니다. Vocab parallel corss entropy loss의 연산은 위와 같이 진행됩니다. 계산된 Logit에서 해당 디바이스가 커버 할 수 있는 부분만 남기고 Masking하여 Loss를 계산합니다. 그리고 Loss를 All-reduce 해서 더합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kevin_env",
   "language": "python",
   "name": "kevin_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
