{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Parallelism\n",
    "- 이번 세션에서는 파이프라인 병렬화에 대해 알아보겠습니다.\n",
    "\n",
    "### 1. Inter-layer model parallelism\n",
    "- 파이프라인 병렬화는 Inter-layer 모델 병렬화를 개선한 것입니다.\n",
    "- Inter-layer 모델 병렬화는 아래와 같이 특정 GPU에 특정 레이어들을 할당하는 모델 병렬화 방법입니다.\n",
    "- 아래 그림에서는 GPU1번에 1,2,3번 레이어가 할당되었고, GPU2번에 4,5번 레이어가 할당 되었습니다.\n",
    "\n",
    "![](../images/inter_layer.png)\n",
    "\n",
    "- 그러나 이전 레이어의 출력을 다음 레이어의 입력으로 하는 신경망의 특성상 특정 GPU의 연산이 끝나야 다른 GPU가 연산을 시작할 수 있습니다.\n",
    "- 즉, 아래의 그림처럼 Inter-layer 모델 병렬화는 동시에 하나의 GPU만 사용할 수 있다는 치명적인 한계를 가지고 있습니다.\n",
    "\n",
    "![](../images/inter_layer_2.png)\n",
    "![](../images/inter_layer_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GPipe\n",
    "- GPipe는 Google에서 개발된 파이프라인 병렬화 기법으로 Inter Layer 모델 병렬화 시 GPU가 쉬는 시간 (idle time)을 줄이기 위해 등장했습니다.\n",
    "- GPipe는 mini-batch를 micro-batch로 한번 더 쪼개서 학습 과정을 파이프라이닝 하는 방식으로 동작합니다.\n",
    "\n",
    "![](../images/gpipe_1.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Micro-batch\n",
    "- Mini-batch는 전체 데이터셋을 n개로 분할한 서브샘플 집합입니다.\n",
    "- Micro-batch는 Mini-batch를 m개로 한번 더 분할한 서브샘플 집합입니다.\n",
    "\n",
    "![](../images/gpipe_2.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### Pipelining\n",
    "- GPipe는 미니배치를 마이크로 배치로 쪼개고 연산을 파이프라이닝 합니다.\n",
    "\n",
    "![](../images/gpipe_3.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPipe with PyTorch\n",
    "- kakaobrain에서 공개한 `torchgpipe`를 사용하면 손쉽게 GPipe를 사용할 수 있습니다.\n",
    "- 단, `nn.Sequential`로 래핑된 모델만 pipeline parallelism이 가능합니다.\n",
    "- 모든 모듈의 입력과 출력 타입은 `torch.Tensor` 혹은 `Tuple[torch.Tensor]`로 제한됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "src/gpipe.py\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchgpipe import GPipe\n",
    "from transformers import GPT2Config, GPT2Tokenizer, GPT2LMHeadModel\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Block as GPT2BlockBase\n",
    "\n",
    "\n",
    "class GPT2Preprocessing(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embed_dim = config.hidden_size\n",
    "        self.wte = nn.Embedding(config.vocab_size, self.embed_dim)\n",
    "        self.wpe = nn.Embedding(config.max_position_embeddings, self.embed_dim)\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        input_shape = input_ids.size()\n",
    "        input_ids = input_ids.view(-1, input_shape[-1])\n",
    "        position_ids = torch.arange(\n",
    "            0, input_shape[-1], dtype=torch.long, device=input_ids.device\n",
    "        )\n",
    "        position_ids = position_ids.unsqueeze(0).view(-1, input_shape[-1])\n",
    "        inputs_embeds = self.wte(input_ids)\n",
    "        position_embeds = self.wpe(position_ids)\n",
    "        hidden_states = inputs_embeds + position_embeds\n",
    "        hidden_states = self.drop(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class GPT2Block(GPT2BlockBase):\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = super(GPT2Block, self).forward(\n",
    "            hidden_states=hidden_states,\n",
    "        )\n",
    "        return hidden_states[0]\n",
    "\n",
    "\n",
    "class GPT2PostProcess(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_f = nn.LayerNorm(\n",
    "            config.hidden_size,\n",
    "            eps=config.layer_norm_epsilon,\n",
    "        )\n",
    "        self.lm_head = nn.Linear(\n",
    "            config.hidden_size,\n",
    "            config.vocab_size,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.ln_f(hidden_states)\n",
    "        lm_logits = self.lm_head(hidden_states)\n",
    "        return lm_logits\n",
    "\n",
    "\n",
    "def create_model_from_pretrained(model_name):\n",
    "    pretrained = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "    preprocess = GPT2Preprocessing(pretrained.config)\n",
    "    preprocess.wte.weight = pretrained.transformer.wte.weight\n",
    "    preprocess.wpe.weight = pretrained.transformer.wpe.weight\n",
    "\n",
    "    blocks = pretrained.transformer.h\n",
    "    for i, block in enumerate(blocks):\n",
    "        block.__class__ = GPT2Block\n",
    "        # 0, 1, 2 => 0\n",
    "        # 3, 4, 5 => 1\n",
    "        # 6, 7, 8 => 2\n",
    "        # 9, 10, 11 => 3\n",
    "\n",
    "    postprocess = GPT2PostProcess(pretrained.config)\n",
    "    postprocess.ln_f.weight = pretrained.transformer.ln_f.weight\n",
    "    postprocess.ln_f.bias = pretrained.transformer.ln_f.bias\n",
    "    postprocess.lm_head.weight.data = pretrained.lm_head.weight.data.clone()\n",
    "\n",
    "    return nn.Sequential(preprocess, *blocks, postprocess)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    world_size = 4\n",
    "    model = create_model_from_pretrained(model_name=\"gpt2\")\n",
    "    model = GPipe(\n",
    "        model,\n",
    "        balance=[4, 3, 3, 4],\n",
    "        devices=[0, 1, 2, 3],\n",
    "        chunks=world_size,\n",
    "    )\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    datasets = load_dataset(\"squad\").data[\"train\"][\"context\"]\n",
    "    datasets = [str(sample) for sample in datasets]\n",
    "    data_loader = DataLoader(datasets, batch_size=8, num_workers=8)\n",
    "\n",
    "    optimizer = Adam(model.parameters(), lr=3e-5)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        optimizer.zero_grad()\n",
    "        tokens = tokenizer(data, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        input_ids = tokens.input_ids.to(0)\n",
    "        labels = tokens.input_ids.to(world_size - 1)\n",
    "\n",
    "        lm_logits = model(input_ids)\n",
    "        shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        loss = nn.CrossEntropyLoss()(\n",
    "            shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"step: {i}, loss: {loss}\")\n",
    "        if i == 300:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/ubuntu/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 316.63it/s]\n",
      "step: 0, loss: 6.088901519775391\n",
      "step: 10, loss: 3.245741367340088\n",
      "step: 20, loss: 2.822031021118164\n",
      "step: 30, loss: 2.5435261726379395\n",
      "step: 40, loss: 2.8309974670410156\n",
      "step: 50, loss: 2.3458638191223145\n",
      "step: 60, loss: 2.5426201820373535\n",
      "step: 70, loss: 2.24003005027771\n",
      "step: 80, loss: 2.474609613418579\n",
      "step: 90, loss: 2.9318113327026367\n",
      "step: 100, loss: 2.8149759769439697\n",
      "step: 110, loss: 2.473766803741455\n",
      "step: 120, loss: 2.9568164348602295\n",
      "step: 130, loss: 2.4007227420806885\n",
      "step: 140, loss: 2.9461541175842285\n",
      "step: 150, loss: 3.9239771366119385\n",
      "step: 160, loss: 3.027210235595703\n",
      "step: 170, loss: 3.0363430976867676\n",
      "step: 180, loss: 1.678962230682373\n",
      "step: 190, loss: 3.5441555976867676\n",
      "step: 200, loss: 3.6646995544433594\n",
      "step: 210, loss: 3.530937433242798\n",
      "step: 220, loss: 2.994187355041504\n",
      "step: 230, loss: 3.1803138256073\n",
      "step: 240, loss: 2.5370934009552\n",
      "step: 250, loss: 3.1481926441192627\n",
      "step: 260, loss: 3.4621517658233643\n",
      "step: 270, loss: 3.2005038261413574\n",
      "step: 280, loss: 2.6140356063842773\n",
      "step: 290, loss: 2.1403472423553467\n",
      "step: 300, loss: 3.4472434520721436\n"
     ]
    }
   ],
   "source": [
    "# !python -m torch.distributed.launch --nproc_per_node=4 ../src/gpipe.py\n",
    "!python ../src/gpipe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. 1F1B Pipelining (PipeDream)\n",
    "\n",
    "- Microsoft에서 공개한 `PipeDream`은 `GPipe`와는 약간 다른 방식의 파이프라이닝을 수행합니다.\n",
    "- 흔히 이 방법을 1F1B라고 부르는데, 모든 Forward가 끝나고 나서 Backward를 수행하는 GPipe와 달리 `PipeDream`은 Forward와 Backward를 번갈아가면서 수행합니다.\n",
    "\n",
    "![](../images/1f1b.png)\n",
    "\n",
    "- 1F1B Pipelining에는 다음과 같은 두가지 챌린지가 존재합니다.\n",
    "  - (1) Weight version managing\n",
    "  - (2) Work partitioning\n",
    "\n",
    "<br>\n",
    "\n",
    "### Weight version managinig\n",
    "- GPipe의 경우 하나의 weight 버전만 운용하지만 주기적으로 Pipeline flush가 일어납니다.\n",
    "- Pipeline flush란 계산된 Gradient를 통해 파라미터를 업데이트 하는 과정.\n",
    "- 이러한 flush 과정 중에는 어떠한 forward, backward 연산도 하지 않기 때문에 처리 효율이 떨어집니다.\n",
    "\n",
    "![](../images/pipeline_flush.png)\n",
    "\n",
    "- PipeDream은 이러한 flush 없이 여러 버전의 파라미터 상태를 지속적으로 관리합니다.\n",
    "- 만약 최신버전의 파라미터만 저장하고 있으면 이전 layer의 출력이 다음 layer로 전송될 때, 다음 layer 부분이 업데이트 될 수도 있습니다.\n",
    "\n",
    "<img src=\"../images/1f1b.gif\" width=800>\n",
    "\n",
    "- 이러한 문제를 막기 위해 여러 버전의 weight를 저장하여 관리합니다.\n",
    "- 그러나 여러버전의 weight를 저장하면 메모리 공간을 많이 차지하게 됩니다.\n",
    "- 따라서 이 부분에서 트레이드 오프가 발생합니다.\n",
    "  - GPipe: 메모리 효율적, 프로세싱 비효율적\n",
    "  - PipeDream: 메모리 비효율적, 프로세싱 효율적\n",
    "  \n",
    "<br>\n",
    "\n",
    "### Work Partitioning\n",
    "- 두번쨰 문제는 뉴럴넷을 어떻게 쪼갤건지에 대한 문제.\n",
    "- 단순히 Layer별로 쪼개는게 답이라고 할 순 없습니다.\n",
    "- 각 파티션의 running time이 비슷해야 idle time을 최소화 할 수 있을 것입니다.\n",
    "- 그 이외에도 parameter size, activation memory 등을 고려해야 합니다.\n",
    "\n",
    "<img src=\"../images/pipe_dream.png\" width=600>\n",
    "\n",
    "- PipeDream은 Profiling과 Optimizing을 통해 최적의 Partioning 전략을 찾아냅니다.\n",
    "- 이는 computing time, parameter size, activation memory 등을 고려합니다.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Variation of 1F1B Pipelining\n",
    "\n",
    "- PipeDream의 1F1B 파이프라이닝을 개선한 두가지 버전의 파이프라인 전략을 소개합니다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 1) PipeDream 2BW (2-buffered weight update)\n",
    "- PipeDream은 메모리 비효율적이였는데, 그것을 해결하기 위해 등장.\n",
    "- 핵심 아이디어는 파이프라이닝 중에 Gradient Accumulation을 수행하는 것.\n",
    "- 여러개의 Gradient들을 모아두다가 한번에 업데이트를 수행하는 방식으로 해결.\n",
    "- 단 두개의 weight version만 유지하면 되고, flush 과정도 필요 없음.\n",
    "\n",
    "![](../images/pipe_dream_2bw.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "### 2) PipeDream Flush: Memory Efficient Pipelining\n",
    "- 1F1B와 Pipeline Flush를 결합한 파이프라이닝 방법\n",
    "- GPIpe와 비교하여 idle time은 비슷하나, forward-backward 과정에서 유지해야 하는 activation 메모리가 줄어서 효율적.\n",
    "- 단일 가중치만 유지하면 되기 때문에 PipeDream 2BW보다도 더 메모리 효율적임.\n",
    "- **DeepSpeed 파이프라인 병렬처리 모듈이 PipeDream Flush로 구현되어 있음.**\n",
    "- https://github.com/microsoft/DeepSpeed/issues/1110\n",
    "\n",
    "![](../images/pipe_dream_flush.png)\n",
    "\n",
    "![](../images/pipe_dream_flush_2.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kevin_env",
   "language": "python",
   "name": "kevin_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
